# -*- coding: utf-8 -*-
"""Parkinson.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZOk9u9x_hGSzinAs89P945UsRv59D0Ve

In this Python machine learning project, we learned to detect the presence of Parkinsonâ€™s Disease in individuals using various factors. We compared XGBClassifier  with many other Modles and checked respective accuracies, made use of the sklearn library to prepare the dataset. This gives us an accuracy of 94.87%, which is great considering the number of lines of code in this python project.
"""

#Importing libraries 

import numpy as np
import pandas as pd
import io
import os, sys
from sklearn.preprocessing import MinMaxScaler
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression


from google.colab import files
uploaded = files.upload()

df= pd.read_csv(io.BytesIO(uploaded['parkinsons.data']))
df.head()

df.info()

df.isnull().sum()

print(features)

scaler=MinMaxScaler((-1,1))
x=scaler.fit_transform(features)
y=labels
print(x)

x_train,x_test,y_train,y_test=train_test_split(x, y, test_size=0.2, random_state=7)

# XGB Booster

model=XGBClassifier()
model.fit(x_train,y_train)
y_pred=model.predict(x_test)
print(accuracy_score(y_test, y_pred)*100)

#Ada Boost Clasifier

from sklearn.ensemble import AdaBoostClassifier

adaboost=AdaBoostClassifier()
adaboost.fit(x_train, y_train)
Y_pred_adaboost = adaboost.predict(x_test)
print(accuracy_score(y_test, Y_pred_adaboost)*100)

# Logistic Regression Method

logistic_regression = LogisticRegression(solver='liblinear',max_iter=1000)
logistic_regression.fit(x_train, y_train)
Y_pred_Logistic = logistic_regression.predict(x_test)
print(accuracy_score(y_test, Y_pred_Logistic)*100)

#Decision Tree Method

tree = DecisionTreeClassifier(random_state=25)
tree.fit(x_train, y_train)
Y_pred_Tree= tree.predict(x_test)
print(accuracy_score(y_test, Y_pred_Tree)*100)

#Support Vector method

from sklearn import svm

clf = svm.SVC(kernel = 'linear')
clf.fit(x_train, y_train)
Y_predict_svm = clf.predict(x_test)
print(accuracy_score(y_test, Y_predict_svm)*100)

#Naive Bayes Gauassian

from sklearn.naive_bayes import GaussianNB

gaussian = GaussianNB()
gaussian.fit(x_train, y_train)
Y_pred_gaussian = gaussian.predict(x_test)
print(accuracy_score(y_test, Y_pred_gaussian)*100)

#stochastic gradient descent Method

from sklearn.linear_model import SGDClassifier
from sklearn.metrics import accuracy_score

sgd_clf = SGDClassifier(random_state=42, max_iter=1000, tol=1e-3)
sgd_clf.fit(x_train, y_train)
Y_pred_SGD = sgd_clf.predict(x_test)
#print("the train score of SGD = ",round(sgd_clf.score(x_train, y_train) *100, 2),"%")
print(accuracy_score(y_test, Y_pred_SGD)*100)

# K Neighbour method

from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(x_train, y_train)
Y_pred_KNN= knn.predict(x_test)
#print("the score of prediction = ",round(knn.score(x_train, y_train) * 100,2), "%")
print(accuracy_score(y_test, Y_pred_KNN)*100)